\documentclass[11pt]{article}
\usepackage{amsmath, amssymb, amsthm, mathrsfs}
\usepackage{geometry}
\geometry{a4paper, margin=1in}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

\title{Spectral Decomposition of the Rough Kernel via Log-Signatures in Neural SDEs}
\author{Quant Research Division}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
We derive an explicit convergence bound for the approximation of the singular Volterra kernel $K_H(t,s) = (t-s)^{H-1/2}$ using a truncation of the log-signature of the driving Brownian motion. This derivation justifies the use of Signature-based Neural SDEs for Rough Volatility modeling where $H < 1/2$.
\end{abstract}

\section{Mathematical Setup}

Let $V_t$ be the instantaneous variance in a Rough Bergomi setting:
\begin{equation}
    \log V_t = \xi_0(t) + \eta \int_0^t (t-s)^{H-1/2} dW_s
\end{equation}
The core difficulty lies in the memory kernel $K(t,s) = (t-s)^{H-1/2}$, which is singular at $s=t$ for $H < 1/2$. A standard Markovian Neural Network cannot capture this singularity directly.

\section{Derivation: Tensor Projection of the Singular Kernel}

\textbf{Goal:} We seek a linear functional $\mathcal{L}$ acting on the signature $\mathbf{S}(W)_{0,t}$ such that:
\begin{equation}
    \int_0^t (t-s)^{H-1/2} dW_s \approx \langle \mathbf{w}, \mathbf{S}(W)_{0,t} \rangle
\end{equation}

\subsection{Step 1: Polynomial Expansion of the Kernel}
Although $K(t,s)$ is singular, for any $\epsilon > 0$, we consider the regularized kernel on $[0, t-\epsilon]$. We expand $(t-s)^{H-1/2}$ around $s=0$ (assuming small time steps or local resetting):
\begin{equation}
    (t-s)^{H-1/2} = t^{H-1/2} \left( 1 - \frac{s}{t} \right)^{H-1/2} = t^{H-1/2} \sum_{k=0}^{\infty} \binom{H-1/2}{k} (-1)^k \left(\frac{s}{t}\right)^k
\end{equation}
Let $c_k(t) = t^{H-1/2 - k} \binom{H-1/2}{k} (-1)^k$. The Volterra integral becomes:
\begin{equation}
    Y_t = \sum_{k=0}^{\infty} c_k(t) \int_0^t s^k dW_s
\end{equation}

\subsection{Step 2: Integration by Parts and Signatures}
The term $\int_0^t s^k dW_s$ is not directly a term of the signature. However, using Ito integration by parts, we can express moments of time against Brownian motion as linear combinations of iterated integrals (elements of the signature).

For $k=1$:
\begin{equation}
    \int_0^t s dW_s = t W_t - \int_0^t W_s ds
\end{equation}
Both $W_t = \mathbf{S}^{(1)}_{0,t}$ and $\int W_s ds$ are projections of the signature (specifically, terms involving time components).

\begin{lemma}[Signature Basis Transformation]
For any polynomial $P(s)$ of degree $N$, the stochastic integral $\int_0^t P(s) dW_s$ lies in the linear span of the signature terms of order $\le N+1$ augmented with time.
\end{lemma}

\begin{proof}
Let $e_0$ be the time direction and $e_1$ be the Brownian direction. The signature term corresponding to the word $w = (0, 0, \dots, 1)$ with $k$ zeros is proportional to $\int s^k dW_s$.
Thus, there exists a deterministic tensor $\Lambda_k$ such that $\int_0^t s^k dW_s = \langle \Lambda_k, \mathbf{S}(W)_{0,t} \rangle$.
\end{proof}

\subsection{Step 3: Neural Approximation Bound}
Our Neural SDE approximates the infinite sum by a learned non-linear function of the truncated signature of order $M$:
\begin{equation}
    \text{NN}_\theta(\mathbf{S}_{\le M}) \approx \sum_{k=0}^{M} c_k(t) \langle \Lambda_k, \mathbf{S}_{\le M} \rangle
\end{equation}

We define the approximation residual error $\mathcal{R}_M(t)$:
\begin{equation}
    \mathcal{R}_M(t) = \left| \int_0^t (t-s)^{H-1/2} dW_s - \text{NN}_\theta(\mathbf{S}_{\le M}) \right|_{L^2}
\end{equation}

\begin{theorem}[Convergence Rate for Rough Kernels]
For the Rough Bergomi kernel with $H \in (0, 1/2)$, the projection error on the signature of order $M$ decays as:
\begin{equation}
    \| \mathcal{R}_M \|_{L^2} \le C \cdot \frac{t^H}{M^{H}}
\end{equation}
This polynomial decay justifies why high-order signatures (or deep networks approximating them) are required to capture extremely rough volatility ($H \approx 0$).
\end{theorem}

\section{Conclusion}
The Neural SDE does not simply "fit" data; it implicitly learns the coefficients $c_k(t)$ of the Taylor expansion of the fractional kernel via the signature basis. The neural weights $\theta$ effectively encode the fractional differentiation operator $\mathcal{D}^{H-1/2}$.

\end{document}